# -*- coding: utf-8 -*-
"""functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RQesrdCMQy8GUYWo865svDL1T2LYn6mc
"""
import matplotlib.pyplot as plt
import pandas as pd
import pickle
import numpy as np
import re
import nltk; nltk.download('punkt'); nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.snowball import EnglishStemmer
from nltk import word_tokenize
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from statsmodels.tsa.base.datetools import dates_from_str
from statsmodels.tsa.api import VAR
from sklearn.decomposition import PCA
import warnings

stemmer = EnglishStemmer()
english_stopwords = set(stopwords.words('english'))

def filter_dataframe_keeping_dates(df, bool_filter, df_day, fill_dict={'cleaned_body': '', 'body': ''}):
  filtered_df = df.loc[bool_filter].reset_index(drop = True)
  filtered_df = filtered_df.join(df_day.set_index('date')[[]], 
                                 on='tweet_dt', how='right').fillna(fill_dict).reset_index(drop=True)
  return filtered_df

def pkl_save(name, obj):
  path = f'/content/drive/MyDrive/DDE/{name}.pkl'
  with open(path, 'wb') as p:
    pickle.dump(obj, p)

def pkl_load(name):
  path = f'/content/drive/MyDrive/DDE/{name}.pkl'
  with open(path, 'rb') as p:
    obj = pickle.load(p)
  return obj


def downscale(signal, df_day, components = 20):
  index = df_day[df_day.year < '2015'].index
  pca = PCA(n_components = components)
  pca.fit(signal[index])
  return pca.transform(signal)

def downscale_pca(signal, df_day, components = 20):
  index = df_day[df_day.year < '2015'].index
  pca = PCA(n_components = components)
  pca.fit(signal[index])
  return pca.transform(signal), pca

def downscale_montly(signal, components = 10):
  month_series = pd.Series(map(lambda x: x[:4], l_dates))
  index = month_series[month_series <= '2015'].index
  pca = PCA(n_components = components)
  pca.fit(signal[index])
  return pca.transform(signal)

def aggregate_signal(signal_matrix, df_time, agg_columns):
  l = []
  for date, group in df_time.groupby(agg_columns):
    means_row = np.mean(signal_matrix[group.index], axis = 0)
    l+= [means_row]
  return np.vstack(l)

def fill_missing_data(df, df_inflation):
  return df_inflation.reset_index()[['DATE']].join(df.set_index('DATE'), on='DATE').fillna(method='ffill')

def tokenizer_stemmer(text):
  text = ' '.join([word for word in text.split() if not word.startswith('http')] )
  pattern = re.compile(r'[^a-z@#&]+')
  lowercase_text = text.lower()
  cleaned_text = pattern.sub(' ', lowercase_text).strip() 
  tokenized_text = cleaned_text.split(" ") #Divide text in tokens

  stopped_tokens = [token for token in tokenized_text if not token in english_stopwords and 
                    not token.startswith('#') and 
                    not token.startswith('@') and 
                    not token.startswith('&') and 
                    not token=='rt' ]
                     # remove stop words from tokens
  meaningful_tokens = stopped_tokens 
  stemmed_tokens = [stemmer.stem(token) for token in meaningful_tokens]
  return stemmed_tokens